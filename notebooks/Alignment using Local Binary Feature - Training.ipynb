{
 "metadata": {
  "name": "",
  "signature": "sha256:4749a2c26d66d809270fde222586bb82e69f0af6a2b0c547ca2c8615db230a50"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from PythonWrapper.tree_based_regression import *\n",
      "%pylab inline\n",
      "\n",
      "%load_ext cythonmagic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def data_augmentation(images, landmarks, bounding_boxes, R):\n",
      "    import random\n",
      "\n",
      "    normalized_landmarks = []\n",
      "    for face, shape in zip(bounding_boxes, landmarks):\n",
      "        normalized_landmarks.append((shape - face[:2]) / (face[2:] - np.asarray(face[:2])))\n",
      "\n",
      "    s0 = []\n",
      "    s_star = []\n",
      "    imgs = []\n",
      "    faces = []\n",
      "    for i in range(len(images)):\n",
      "        face = bounding_boxes[i]\n",
      "        s0 += [s * (face[2:] - np.asarray(face[:2])) + face[:2] for s in random.sample(normalized_landmarks, R)]\n",
      "        s_star += [landmarks[i]]*R\n",
      "        imgs += [images[i]]*R\n",
      "        faces += [bounding_boxes[i]]*R\n",
      "        \n",
      "    return np.asarray(imgs), np.asarray(s0), np.asarray(s_star), faces"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# BioID"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import BioId\n",
      "\n",
      "training_set_size = 900\n",
      "\n",
      "dataset = BioId()\n",
      "images = dataset.loadImages()[:training_set_size]\n",
      "ground_truth = dataset.loadGroundTruth()[:training_set_size]\n",
      "bounding_boxes = dataset.loadBoundingBoxes()[:training_set_size]\n",
      "\n",
      "print \"Number of test images: %d\"%training_set_size"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Number of test images: 900\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "T = 5\n",
      "N = 300 / 20\n",
      "D = 5\n",
      "R = 20"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)   \n",
      "print \"Total training data after data augmentation: %d\"%len(s0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining, node_separation_criterias\n",
      "\n",
      "local_trees_training = AlignmentMethodTraining(R, T, N, D)\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_trees_training.train(s0, s_star, faces, imgs, (0,1), \"local_trees_regression_model_bioid.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbf_training.train(s0, s_star, faces, imgs, (0,1), \"lbf_regression_model_bioid.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 300-W"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 68 landmarks with dataset face detector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadTrainingSet()\n",
      "\n",
      "print \"Size of training set: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 60\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Total training data after data augmentation: %d\"%len(s0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of training set: 3148\n",
        "Total training data after data augmentation: 62960"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining, node_separation_criterias\n",
      "\n",
      "local_trees_training = AlignmentMethodTraining(R, T, N, D)\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for criteria in node_separation_criterias.keys():\n",
      "    print \"Training using \" + criteria + \" criteria...\"\n",
      "    local_trees_training.train(s0, s_star, faces, imgs, (36,45), \"models/alignment/criterias/local_trees_regression_model_300w_\" + criteria.replace(\" \",\"_\") + \".txt\", criteria)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training using var criteria...\n",
        "Training using mean norm criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training using normalized var criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training using ls criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training using normalized ls criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_trees_training.train(s0, s_star, faces, imgs, (36,45), \"local_trees_regression_model_300w.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for criteria in node_separation_criterias.keys():\n",
      "    print \"Training using \" + criteria + \" criteria...\"\n",
      "    lbf_training.train(s0, s_star, faces, imgs, (36,45), \"models/alignment/criterias/lbf_regression_model_300w_\" + criteria.replace(\" \",\"_\") + \".txt\", criteria)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training using var criteria...\n",
        "Training using mean norm criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training using normalized var criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training using ls criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Training using normalized ls criteria..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbf_training.train(s0, s_star, faces, imgs, (36,45), \"lbf_regression_precise_model_300w.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 68 landmarks with OpenCV face detector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadTrainingSet(detector = \"opencv\")\n",
      "\n",
      "print \"Size of training set: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 60\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Total training data after data augmentation: %d\"%len(s0)\n",
      "\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining, node_separation_criterias\n",
      "\n",
      "local_trees_training = AlignmentMethodTraining(R, T, N, D)\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of training set: 2099\n",
        "Total training data after data augmentation: 41980"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_trees_training.train(s0, s_star, faces, imgs, (36,45), \"local_trees_regression_model_300w_opencv_detector.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbf_training.train(s0, s_star, faces, imgs, (36,45), \"lbf_regression_model_300w_opencv_detector.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 51 landmarks with dataset face detector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadTrainingSet(contour_landmarks = False)\n",
      "\n",
      "print \"Training set size: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 50\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Training set size after data augmentation: %d\"%len(s0)\n",
      "\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining, node_separation_criterias\n",
      "\n",
      "local_trees_training = AlignmentMethodTraining(R, T, N, D)\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set size: 3148\n",
        "Training set size after data augmentation: 62960"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_trees_training.train(s0, s_star, faces, imgs, (19,28), \"local_trees_regression_model_300w_51_landmarks.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbf_training.train(s0, s_star, faces, imgs, (19,28), \"lbf_regression_model_300w_51_landmarks.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 51 landmarks with OpenCV face detector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadTrainingSet(detector = \"opencv\", contour_landmarks = False)\n",
      "\n",
      "print \"Training set size: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 50\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Training set size after data augmentation: %d\"%len(s0)\n",
      "\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining, node_separation_criterias\n",
      "\n",
      "local_trees_training = AlignmentMethodTraining(R, T, N, D)\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training set size: 2117\n",
        "Training set size after data augmentation: 42340"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local_trees_training.train(s0, s_star, faces, imgs, (19,28), \"local_trees_regression_model_300w_51_landmarks_opencv_detector.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lbf_training.train(s0, s_star, faces, imgs, (19,28), \"lbf_regression_model_300w_51_landmarks_opencv_detector.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Final learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 68 landmarks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadCompleteDataset(detector = \"opencv\")\n",
      "\n",
      "print \"Size of training set: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 60\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Total training data after data augmentation: %d\"%len(s0)\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")\n",
      "lbf_training.train(s0, s_star, faces, imgs, (36,45), \"lbf_regression_model_68_landmarks.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of training set: 2558\n",
        "Total training data after data augmentation: 51160"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 51 landmarks"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadCompleteDataset(detector = \"opencv\", contour_landmarks = False)\n",
      "\n",
      "print \"Size of training set: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 50\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Total training data after data augmentation: %d\"%len(s0)\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")\n",
      "lbf_training.train(s0, s_star, faces, imgs, (19,28), \"lbf_regression_model_51_landmarks.txt\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of training set: 2558\n",
        "Total training data after data augmentation: 51160"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Perfect detector"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadCompleteDataset(detector = \"perfect\")\n",
      "\n",
      "print \"Size of training set: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 60\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Total training data after data augmentation: %d\"%len(s0)\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")\n",
      "lbf_training.train(s0, s_star, faces, imgs, (36,45), \"lbf_regression_model_68_landmarks_perfect_detector.txt\")\n",
      "\n",
      "del images, ground_truth, bounding_boxes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of training set: 3837\n",
        "Total training data after data augmentation: 76740"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from landmarks_datasets import Dataset300W\n",
      "\n",
      "dataset = Dataset300W()\n",
      "images, ground_truth, bounding_boxes = dataset.loadCompleteDataset(detector = \"perfect\", contour_landmarks = False)\n",
      "\n",
      "print \"Size of training set: %d\"%images.shape[0]\n",
      "\n",
      "T = 5\n",
      "N = 300 / 50\n",
      "D = 5\n",
      "R = 20\n",
      "\n",
      "imgs, s0, s_star, faces = data_augmentation(images, ground_truth, bounding_boxes, R)\n",
      "print \"Total training data after data augmentation: %d\"%len(s0)\n",
      "\n",
      "from PythonWrapper.tree_based_regression import AlignmentMethodTraining\n",
      "lbf_training = AlignmentMethodTraining(R, T, N, D, method=\"lbf\")\n",
      "lbf_training.train(s0, s_star, faces, imgs, (19,28), \"lbf_regression_model_51_landmarks_perfect_detector.txt\")\n",
      "\n",
      "del images, ground_truth, bounding_boxes"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Size of training set: 3837\n",
        "Total training data after data augmentation: 76740"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}